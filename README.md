# PySpark Aspire

**Aspire + JupyterLab & PySpark**  

Combining the power and elegance of the Aspire framework with JupyterLab & PySpark for fast, noâ€‘friction development and testing.

---

## ğŸš€ Project Goals

- **Combine Aspire + JupyterLab**: Provide a simple, quick, noâ€‘friction local dev experience by combining Aspire with JupyterLab notebooks.
- **Preâ€‘loaded Azure libraries**: Ship a local environment with PySpark and the Azure SDK for Python available out of the box so you can prototype quickly.
- **Local Azure Blob integration**: Provide easy local Blob storage via the official Azurite emulator.
- **Local Cosmos DB integration**: Provide easy local Cosmos DB via the official Cosmos DB (Preview) emulator.
- **Local Kafka, Kafka Connect, Schema Registry** etc.

- **Deployment readiness**: While this project focuses on **local development**, the same notebooks can be adapted for deployment to **Azure Synapse** or **Microsoft Fabric** environments.

---

## ğŸ“‹ Prerequisites

- Install **.NET 10 SDK** (required to build/run Aspire components).  
  ğŸ‘‰ [Download .NET SDK](https://dotnet.microsoft.com/download)
- Install the latest **Aspire framework** as per the [Aspire documentation](https://aspire.dev/).
- **Docker** (for building and running the custom container).
- *(Optional)* **Azure Storage Explorer** â€” recommended for copying files into Azurite.  
  ğŸ‘‰ [Download Storage Explorer](https://azure.microsoft.com/features/storage-explorer/)

---

## ğŸ“‚ Files & Folders of Interest

- `data/hr.csv` â€” sample HR data used by Azurite-Sample.ipynb notebook.
- `pyspark-notebooks/` â€” project notebooks; this folder is mapped into JupyterLab for convenience.

---

## ğŸ³ Build the Docker Image

From the repository root, build the custom container used for the **JupyterLab + PySpark + Azure SDK** environment:

```bash
docker build -t pyspark-azure-notebook .
```

## â–¶ï¸ Run Aspire (Start Aspire, Azurite & JupyterLab)

This project leverages the **Aspire framework** to orchestrate your local development environment.  
Follow these steps to get everything running smoothly:

1. ğŸ“¦ **Restore dependencies**
    ```bash
   dotnet restore
    ```

2. ğŸ“¦ **Build the Aspire project**
    ```bash
   dotnet build
    ```

3. ğŸ“¦ **Run Aspire**
    ```bash
   aspire run
    ```

### ğŸš€ What Happens When You Run Aspire

Running `aspire run` will:

- ğŸŒ Launch the **Aspire application** and open the **Aspire Dashboard** in your browser.  
- ğŸ“¦ Start the local **Azurite & Cosmos DB emulators** automatically.  
- ğŸ“’ Spin up the **Kafka, Kafka Connect, Schema Registry containers**. 
- ğŸ“’ Spin up the **JupyterLab container**.  

---

### ğŸ“¥ Load Sample Data into Azurite

After Aspire has started:

1. ğŸ”— Use **Azure Storage Explorer** (recommended) to connect to the local Azurite instance.  
2. ğŸ“‚ Copy `data/hr.csv` into a container named **`data`**.  
3. âœ… Your notebooks can now access blobs directly from Azurite for testing and prototyping.  


## ğŸ“ Notes & Recommendations

- ğŸ“’ **Project notebooks**: The `pyspark-notebooks` folder is bound to JupyterLab at `/home/jovyan/work` for convenience â€” edit locally and work in Jupyter seamlessly.
- ğŸ“‚ **Data mounting**: The `data/` folder lets notebooks read `data/hr.csv` directly or access blobs through the Azurite endpoint.
- ğŸŒ **Azurite endpoint**: If you need the blob endpoint URL for notebooks, it will typically be:
`http://azure-storage:10000`

Configure your connection strings in notebooks to point to this endpoint when running against Azurite.

---

## âœ… Quick Checklist

- ğŸ”§ [.NET 10 SDK installed]  
- ğŸ—ï¸ [Aspire framework installed]  
- ğŸ³ [Build Docker image with `docker build -t pyspark-azure-notebook .`]  
- ğŸ“¦ [Copy `data/hr.csv` into a `data` container using Azure Storage Explorer]  